# -*- coding: utf-8 -*-
"""24-25+psl+Ayush+R1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ox6PjtxuHcWgiWbOkQjMqzmXoZrrhSGB
"""

"""
HYBRID PDF to Excel Extractor - NO NAME, NO COLLEGE NAME VERSION
‚úÖ Extracts: Sr. No, AIR, NEET Roll, CET Form, Gender, Category, Quota, College Code only
‚úÖ Fixed all missing rows issues
"""

!pip install pdfplumber openpyxl pandas

import pdfplumber
import pandas as pd
import re
from google.colab import files
import time

print("PDF upload karo:")
uploaded = files.upload()
pdf_filename = list(uploaded.keys())[0]

# X-axis coordinates for columns
GENDER_X = {'x0': 422.57, 'x1': 427.36}
CAT_X = {'x0': 434.97, 'x1': 456.97}
QUOTA_X = {'x0': 492.56, 'x1': 514.56}
COLLEGE_CODE_X = {'x0': 624.14, 'x1': 643.32}

def extract_column_by_coordinates(words, x0, x1, y_pos, tolerance=5):
    """Extract text at specific coordinates and y-position"""
    found_words = []

    for word in words:
        word_x0 = word['x0']
        word_x1 = word['x1']
        word_y = word['top']

        if abs(word_y - y_pos) > tolerance:
            continue

        if (x0 - tolerance <= word_x0 <= x1 + tolerance) or \
           (x0 - tolerance <= word_x1 <= x1 + tolerance) or \
           (word_x0 <= x0 and word_x1 >= x1):
            found_words.append(word['text'])

    return ' '.join(found_words) if found_words else ''

def is_footer_line(line):
    """Check if line is footer/header"""
    upper_line = line.upper().strip()

    footer_keywords = [
        'GOVERNMENT', 'STATE COMMON', 'ADMISSIONS TO',
        'PROVISIONAL', 'NOTE:', 'PRINTED ON', 'LEGENDS',
        'SR.', 'AIR', 'NEET', '---', 'NO. ROLL',
        'CAP ROUND', 'MAHARASHTRA', 'HEALTH SCIENCE',
        'LAST DATE', 'ADMITTING', 'SELECTION LIST'
    ]

    if any(upper_line.startswith(kw) for kw in footer_keywords):
        return True

    if line.count('-') > 20:
        return True

    header_indicators = ['SR.', 'AIR', 'NEET', 'CET', 'FORM.', 'NAME', 'CAT.']
    if sum(1 for kw in header_indicators if kw in upper_line) >= 4:
        return True

    return False

def find_y_coordinate_for_sr_no(words, sr_no):
    """Find the y-coordinate (vertical position) for a given Sr. No"""
    for word in words:
        if word['text'] == sr_no and word['x0'] < 100:
            return word['top']
    return None

def process_accumulated_line(accumulated_line, words, sr_to_y, page_num):
    """Process a single accumulated line (may span multiple PDF lines)"""
    line = accumulated_line.strip()

    if not line or is_footer_line(line):
        return None

    # ‚úÖ FIXED: Remove AIR digit limit - match ANY number of digits
    match = re.match(r'^\s*(\d+)\s+(\d+)\s+(\d+)\s+(\d+)\s+(.+)', line)
    if not match:
        return None

    sr_no = match.group(1)
    air = match.group(2)
    neet_roll = match.group(3)
    cet_form = match.group(4)
    rest = match.group(5).strip()

    # Debug logging for specific rows
    debug_rows = ['44', '816', '1118', '2135', '2356']
    if sr_no in debug_rows:
        print(f"\nüîç DEBUG Sr.No {sr_no}:")
        print(f"   AIR: {air} (length: {len(air)})")
        print(f"   Rest: {rest[:100]}...")

    # ‚úÖ FIXED: More lenient validation
    try:
        sr_int = int(sr_no)
        air_int = int(air)
        if sr_int > 100000:
            if sr_no in debug_rows:
                print(f"   ‚ùå SKIPPED: Sr.No too high")
            return None
    except:
        if sr_no in debug_rows:
            print(f"   ‚ùå SKIPPED: Validation error")
        return None

    # ‚úÖ FIND Y-COORDINATE for this Sr. No
    row_y = sr_to_y.get(sr_no)
    if not row_y:
        row_y = find_y_coordinate_for_sr_no(words, sr_no)

    if not row_y:
        # FALLBACK: Use text parsing
        gender_matches = list(re.finditer(r'(?:^|\s)([MF])(?:\s|$)', rest))
        if not gender_matches:
            if sr_no in debug_rows:
                print(f"   ‚ùå FALLBACK: No gender found")
            return None

        gender_match = gender_matches[0]
        gender = gender_match.group(1)

        after_gender = rest[gender_match.end():].strip()
        parts = after_gender.split()

        category = parts[0] if parts else ''
        quota = parts[1] if len(parts) > 1 else ''
        college_code = parts[2] if len(parts) > 2 else ''

        # Extract only numeric college code
        if college_code:
            code_match = re.match(r'^(\d+)', college_code)
            if code_match:
                college_code = code_match.group(1)
            else:
                college_code = ''

        if sr_no in debug_rows:
            print(f"   ‚úÖ FALLBACK: Added")

        return {
            'Sr. No': sr_no,
            'AIR': air,
            'NEET Roll No': neet_roll,
            'CET Form No': cet_form,
            'Gender': gender,
            'Category': category,
            'Quota': quota,
            'College Code': college_code,
            'Page': page_num,
            'Method': 'Text Parsing (Fallback)'
        }

    # ‚úÖ IMPROVED: Better gender detection
    gender_matches = list(re.finditer(r'(?:^|\s)([MF])(?:\s|$)', rest))

    if not gender_matches:
        if sr_no in debug_rows:
            print(f"   ‚ùå SKIPPED: No gender marker found")
        return None

    gender_match = gender_matches[0]

    if sr_no in debug_rows:
        print(f"   ‚úÖ Gender: {gender_match.group(1)}")

    # ‚úÖ USE COORDINATE-BASED EXTRACTION
    gender = extract_column_by_coordinates(words, GENDER_X['x0'], GENDER_X['x1'], row_y)
    category = extract_column_by_coordinates(words, CAT_X['x0'], CAT_X['x1'], row_y)
    quota = extract_column_by_coordinates(words, QUOTA_X['x0'], QUOTA_X['x1'], row_y)
    college_code = extract_column_by_coordinates(words, COLLEGE_CODE_X['x0'], COLLEGE_CODE_X['x1'], row_y)

    # Clean college code - only keep numeric part
    if college_code:
        code_match = re.match(r'^(\d+)', college_code)
        if code_match:
            college_code = code_match.group(1)
        else:
            college_code = ''

    # Clean gender (should be M or F)
    if gender:
        gender = gender[0] if gender[0] in ['M', 'F'] else ''

    # Only add if we have at least gender
    if gender:
        if sr_no in debug_rows:
            print(f"   ‚úÖ‚úÖ ADDED to results!")
            print(f"   Category: '{category}', Quota: '{quota}', College Code: '{college_code}'")

        return {
            'Sr. No': sr_no,
            'AIR': air,
            'NEET Roll No': neet_roll,
            'CET Form No': cet_form,
            'Gender': gender,
            'Category': category,
            'Quota': quota,
            'College Code': college_code,
            'Page': page_num,
            'Method': 'Coordinate-Based'
        }
    else:
        if sr_no in debug_rows:
            print(f"   ‚ùå SKIPPED: Missing gender")
        return None

def process_page(page_num, pdf_path):
    """Process a single page - FIXED HYBRID approach with line accumulation"""
    page_data = []

    try:
        with pdfplumber.open(pdf_path) as pdf:
            page = pdf.pages[page_num - 1]

            text = page.extract_text()
            if not text:
                return page_data

            words = page.extract_words()

            # Build Sr. No -> y-coordinate lookup
            sr_to_y = {}
            for word in words:
                if word['text'].isdigit() and word['x0'] < 100:
                    sr_to_y[word['text']] = word['top']

            lines = text.split('\n')

            # ‚úÖ NEW: Accumulate lines that belong together
            accumulated_line = ""

            for i, line in enumerate(lines):
                line = line.strip()

                if not line:
                    continue

                # Check if this line starts with Sr. No (new record)
                if re.match(r'^\s*(\d+)\s+(\d+)\s+(\d+)\s+(\d+)', line):
                    # Process previous accumulated line
                    if accumulated_line:
                        record = process_accumulated_line(accumulated_line, words, sr_to_y, page_num)
                        if record:
                            page_data.append(record)

                    # Start new accumulation
                    accumulated_line = line
                else:
                    # This might be a continuation line
                    if accumulated_line and not is_footer_line(line):
                        accumulated_line += " " + line

            # Process last accumulated line
            if accumulated_line:
                record = process_accumulated_line(accumulated_line, words, sr_to_y, page_num)
                if record:
                    page_data.append(record)

    except Exception as e:
        print(f"Error on page {page_num}: {str(e)}")

    return page_data

# Main processing
print("\n" + "="*80)
print("FIXED PDF EXTRACTOR - NO NAME, NO COLLEGE NAME")
print("="*80)

with pdfplumber.open(pdf_filename) as pdf:
    total_pages = len(pdf.pages)

print(f"Total pages: {total_pages}\n")

# Test first page
print("Testing first page...")
test_data = process_page(1, pdf_filename)
print(f"Page 1: {len(test_data)} records\n")

if len(test_data) > 0:
    print("Sample from page 1:")
    for record in test_data[:3]:
        print(f"  Sr.{record['Sr. No']}: Gender={record['Gender']}, AIR={record['AIR']}")
        print(f"    Category: {record['Category']}, Quota: {record['Quota']}, College: {record['College Code']}")
    print()

# Process all pages
print("Processing all pages...\n")
start_time = time.time()
all_data = []

for page_num in range(1, total_pages + 1):
    data = process_page(page_num, pdf_path=pdf_filename)
    all_data.extend(data)

    if page_num % 5 == 0 or page_num == total_pages:
        print(f"Processed {page_num}/{total_pages} pages... ({len(all_data)} records)")

end_time = time.time()

# Create DataFrame
df = pd.DataFrame(all_data)

if len(df) > 0:
    df['Sr. No'] = pd.to_numeric(df['Sr. No'], errors='coerce')
    df = df.sort_values('Sr. No').reset_index(drop=True)

# ‚úÖ CHECK FOR MISSING ROWS
print("\n" + "="*80)
print("MISSING ROWS ANALYSIS")
print("="*80)

if len(df) > 0:
    extracted_sr_nos = set(df['Sr. No'].dropna().astype(int).tolist())

    if extracted_sr_nos:
        min_sr = min(extracted_sr_nos)
        max_sr = max(extracted_sr_nos)
        expected_sr_nos = set(range(min_sr, max_sr + 1))
        missing_sr_nos = sorted(expected_sr_nos - extracted_sr_nos)

        if missing_sr_nos:
            print(f"‚ö†Ô∏è  MISSING Sr. Numbers: {len(missing_sr_nos)} rows")
            print(f"\nMissing Sr. No list:")

            missing_groups = []
            start = missing_sr_nos[0]
            end = missing_sr_nos[0]

            for i in range(1, len(missing_sr_nos)):
                if missing_sr_nos[i] == end + 1:
                    end = missing_sr_nos[i]
                else:
                    if start == end:
                        missing_groups.append(f"{start}")
                    else:
                        missing_groups.append(f"{start}-{end}")
                    start = missing_sr_nos[i]
                    end = missing_sr_nos[i]

            if start == end:
                missing_groups.append(f"{start}")
            else:
                missing_groups.append(f"{start}-{end}")

            print(f"  {', '.join(missing_groups)}")

            print(f"\nFirst {min(20, len(missing_sr_nos))} missing rows:")
            for sr in missing_sr_nos[:20]:
                print(f"  - Sr. No: {sr}")
        else:
            print("‚úÖ No missing rows! All Sr. Numbers extracted sequentially.")
    else:
        print("‚ùå No valid Sr. Numbers found in data")
else:
    print("‚ùå No data extracted")

# Results
print("\n" + "="*80)
print("EXTRACTION COMPLETE")
print("="*80)
print(f"Time: {end_time - start_time:.2f} seconds")
print(f"Records extracted: {len(df)}")
print(f"Expected: ~{total_pages * 35}")
if total_pages > 0:
    print(f"Success rate: {(len(df) / (total_pages * 35) * 100):.1f}%")

# Method breakdown
if len(df) > 0:
    print("\n" + "="*80)
    print("EXTRACTION METHOD BREAKDOWN")
    print("="*80)
    method_counts = df['Method'].value_counts()
    for method, count in method_counts.items():
        print(f"  {method}: {count} records")

# Show sample
if len(df) > 0:
    print("\n" + "="*80)
    print("SAMPLE DATA (First 10 records)")
    print("="*80)
    sample_df = df.head(10)[['Sr. No', 'Gender', 'Category', 'Quota', 'College Code']]
    print(sample_df.to_string(index=False))

# Category & Quota Analysis
if len(df) > 0:
    print("\n" + "="*80)
    print("CATEGORY & QUOTA ANALYSIS")
    print("="*80)

    print("\nUnique Categories:")
    cat_counts = df['Category'].value_counts()
    for cat, count in cat_counts.head(15).items():
        print(f"  '{cat}': {count}")

    print("\nUnique Quotas:")
    quota_counts = df['Quota'].value_counts()
    for quota, count in quota_counts.head(15).items():
        print(f"  '{quota}': {count}")

    print(f"\nRecords with empty Category: {(df['Category'] == '').sum()}")
    print(f"Records with empty Quota: {(df['Quota'] == '').sum()}")

# Data Quality Check
print("\n" + "="*80)
print("DATA QUALITY CHECK")
print("="*80)

if len(df) > 0:
    print(f"Missing Gender: {(df['Gender'] == '').sum()}")
    print(f"Missing Category: {(df['Category'] == '').sum()}")
    print(f"Missing Quota: {(df['Quota'] == '').sum()}")
    print(f"Missing College Code: {(df['College Code'] == '').sum()}")

# Save to Excel
excel_filename = 'NEET_Final_No_Name_No_College_Name.xlsx'
print(f"\n{'='*80}")
print("SAVING TO EXCEL")
print("="*80)

if len(df) > 0:
    df_export = df[['Sr. No', 'AIR', 'NEET Roll No', 'CET Form No', 'Gender', 'Category', 'Quota', 'College Code', 'Page']]
    df_export.to_excel(excel_filename, index=False, engine='openpyxl')
    print(f"‚úì Saved: {excel_filename}")
    print(f"‚úì Total records: {len(df)}")
    files.download(excel_filename)
else:
    print("‚ùå No data to save!")

print("\n" + "="*80)
print("‚úì DONE!")
print("="*80)









